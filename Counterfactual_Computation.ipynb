{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Counterfactual Computation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CHzqEQW8qZUM",
        "Q7AzwDhvuHX-",
        "nyGDsY2j4NfG"
      ],
      "authorship_tag": "ABX9TyPoaG86TM1l3/mUXSd14zHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahantesh-Pattadkal-1993/CounterFactuals_GANs/blob/main/Counterfactual_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWIBBfXcP_kz",
        "outputId": "a43e8525-824e-4cba-e753-638d2ef3995f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uma5W4TFQSwX"
      },
      "source": [
        "#Setting up the path for GAN Training\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Github/CounterFactuals_GANs/MNIST')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amOxIb6xQVbM",
        "outputId": "c1bc2235-b3fd-496f-930e-1ae91727e094"
      },
      "source": [
        "#Import the libraries \n",
        "import torch\n",
        "import torch.optim as opt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from GAN_Models import DC_Generator, DC_Discriminator, Net, CNN, Net_logits\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 2021\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd6d729c050>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnkacTUZQu_",
        "outputId": "bcf18402-ba5c-4ec6-875e-ae496903c597"
      },
      "source": [
        "#Load the Generator  \n",
        "# using the 150 epochs dataset\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "\n",
        "G = DC_Generator().to(device)\n",
        "D = DC_Discriminator().to(device)\n",
        "\n",
        "\n",
        "checkpoint = torch.load(\"Weights/G_checkpoint_latest_149.pth\", map_location=torch.device('cuda'))\n",
        "G.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "checkpoint = torch.load(\"Weights/D_checkpoint_latest_149.pth\", map_location=torch.device('cuda'))\n",
        "D.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxUuFeUMQdht",
        "outputId": "f09ae4ac-6937-4de4-8b31-69b88479c9ed"
      },
      "source": [
        "#Load the Classifier\n",
        "Classifier_model = Net()\n",
        "Classifier_checkpoint = torch.load(\"Weights/Classifier_CNN.pth\", map_location=torch.device('cuda'))\n",
        "Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model.eval()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QgUV9HIGHoI",
        "outputId": "9ccaca11-9e27-4658-e583-6554e60e63e4"
      },
      "source": [
        "#Load the Classifier Net_logits() and outputs :- softmax, logits\n",
        "Classifier_model_logits = Net_logits().to(device)\n",
        "Classifier_checkpoint = torch.load(\"Weights/Classifier_CNN_logits.pth\", map_location=torch.device('cuda'))\n",
        "Classifier_model_logits.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model_logits.eval()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net_logits(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sur5CGLPYCj4",
        "outputId": "b1ae0602-e3e3-4b5b-dbef-3521ab7545bb"
      },
      "source": [
        "#Load Eoins Classifier\n",
        "Classifier_model_eoin = CNN()\n",
        "Classifier_checkpoint = torch.load(\"Weights/pytorch_cnn.pth\", map_location=torch.device('cpu'))\n",
        "#Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model_eoin.eval()\n",
        "\n",
        "# Gettin weird predictions, maybe the state dict is missing"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout2d(p=0.1, inplace=False)\n",
              "    (4): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Dropout2d(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Dropout2d(p=0.1, inplace=False)\n",
              "    (12): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): Dropout2d(p=0.2, inplace=False)\n",
              "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHnhp6XXRCby",
        "outputId": "20ff7276-a72b-4a9c-adfe-79a577978cc9"
      },
      "source": [
        "#Loading the data\n",
        "\n",
        "mb_size = 2\n",
        "\n",
        "transform = transforms.Compose(\n",
        "\t\t[transforms.ToTensor(),\n",
        "\t\t transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainData = torchvision.datasets.MNIST('./data/', download=True, transform=transform, train=True)\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, shuffle=False, batch_size=mb_size)\n",
        "\n",
        "dataIter = iter(trainLoader)\n",
        "\n",
        "imgs, labels = dataIter.next()\n",
        "imgs = imgs.to(device)\n",
        "\n",
        "print(imgs.shape)\n",
        "print(labels)\n",
        "imgs.is_cuda"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 28, 28])\n",
            "tensor([5, 0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2b9tm8hblrI",
        "outputId": "ffac7a0c-bd67-4550-ec4d-4af26ef8779d"
      },
      "source": [
        "next(Classifier_model_logits.parameters()).device\n",
        "\n",
        "sample_image.is_cuda"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "J4QajikfSHfx",
        "outputId": "e329176b-4c58-4c42-e4e7-814801878c93"
      },
      "source": [
        "sample_image = imgs[1,:,:,:] #first image in this batch \n",
        "sample_image= sample_image.reshape([1,1,28,28]) #adding the dimension for batch size\n",
        "sample_image = sample_image.to('cpu') #pushing it to cpu as that numpy conversion can be done\n",
        "\n",
        "\n",
        "npimgs = sample_image[0].detach().numpy()\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')\n",
        "\n",
        "sample_image = sample_image.to('cuda')\n",
        "output, logits = Classifier_model_logits(sample_image)\n",
        "print(output)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0000e+00, 8.4316e-11, 1.7799e-08, 5.8022e-12, 1.0353e-11, 1.9533e-13,\n",
            "         2.2826e-09, 1.6658e-10, 1.5406e-11, 1.8834e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--otHGKdTbBh"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "kBQFg3WiaO4G",
        "outputId": "ad9c5325-aa7d-4eb3-f7e1-76c7bc2d5109"
      },
      "source": [
        "#find the z_org that respresents the latent representation of given image\n",
        "\n",
        "def latent_representation(z,sample_image,pred):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(sample_image-G(z))).sum() \n",
        "    loss2 = torch.square(torch.abs(Classifier_model_logits(G(z))[0] - Classifier_model_logits(sample_image)[0]).sum())\n",
        "    #loss2 = torch.square(torch.abs(Classifier_model_logits(G(z)) - Classifier_model_logits(sample_image)).sum()) # use when Net() model is used\n",
        "    \n",
        "\n",
        "    loss = loss1 + loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    \n",
        "    if(i==10000):\n",
        "      break\n",
        "\n",
        "  return z\n",
        "\n",
        "z_org = latent_representation(z,sample_image,4)\n",
        "\n",
        "G_img= G(z_org) #generate the img\n",
        "G_img = G_img.to('cpu')  # bring it to cpu\n",
        "npimgs = G_img[0].detach().numpy()  #plot the image\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')\n",
        "\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(208.4463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(22.9522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(208.4463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(16.5105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(22.9522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(14.5849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(16.5105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(13.5970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(14.5849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(12.8471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(13.5970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(12.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(12.8471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(11.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(12.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(10.7907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(11.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(9.9972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(10.7907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(9.6282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor(9.9972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(9.4002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor(9.6282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(9.2269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor(9.4002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(9.0863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor(9.2269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(8.9015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor(9.0863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(8.9059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor(8.9015, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b0acd2abdaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mG_img\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_org\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mnpimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCFW5KPwnBn",
        "outputId": "dffc8707-6684-448f-a014-2f9dca46fe15"
      },
      "source": [
        "Classifier_model_logits(G(z))[0]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3555e-06, 9.3431e-12, 2.9211e-12, 1.9332e-12, 3.1462e-12, 5.2353e-05,\n",
              "         9.9993e-01, 1.9209e-13, 2.0184e-05, 2.9865e-13]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHzqEQW8qZUM"
      },
      "source": [
        "## Latent dimension loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "2FRhTyamqTNh",
        "outputId": "ff6e7abb-3a35-4e47-c94c-93dc95f033f1"
      },
      "source": [
        "#using latent dimension euclidean distance and PR loss\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() #latent dim euclidean loss\n",
        "    loss2 = torch.log(Classifier_model_logits(G(z))[0][0][target_class]) #as classifier gives 2 outputs we take 0th index\n",
        "    loss3 = torch.log(D(G(z))) #plausibility\n",
        "\n",
        "    loss = loss1 - loss2 \n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z, loss1, loss2, loss3\n",
        "\n",
        "target_class = 1\n",
        "z_eucli, loss1, loss2, loss3 = counterfactual_eluclidian(z,z_org,target_class)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z_eucli)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(44.3367, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(11.3798, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(44.3367, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(5.1295, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(11.3798, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(4.1300, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(5.1295, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(3.9595, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(4.1300, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(3.9356, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(3.9595, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(3.9402, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(3.9356, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbb386abb50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFnCruWDtsLu",
        "outputId": "2e2feda7-da48-45e2-d0f4-e3d8aa5b7b91"
      },
      "source": [
        "Classifier_model_logits(G(z_eucli))[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8972e-03, 4.9882e-01, 1.5683e-01, 6.7859e-02, 1.2414e-01, 2.1038e-05,\n",
              "         1.3386e-01, 1.0421e-02, 6.0656e-03, 8.5337e-05]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "0ejFwG9tsNvD",
        "outputId": "b13dbdcd-74fd-449c-fe0a-6e320ba7309b"
      },
      "source": [
        "#using latent dimension distance and without PR loss\n",
        "\n",
        "#This can be used as the baselines that do not talk about plausibility\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() \n",
        "    loss2 = torch.log(Classifier_model_logits(G(z))[0][0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    #loss = loss1 - 10*loss2 - loss3\n",
        "    loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z\n",
        "\n",
        "z = counterfactual_eluclidian(z,z_org,3)\n",
        "\n",
        "#Check if the image generated is close to the real image\n",
        "G_img= G(z)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(12.6871, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f03fab9b310>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCKydwVltbGi"
      },
      "source": [
        "#using latent dimension distance but cosine similarity\n",
        "import torch\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "def counterfactual_cosine(z,z_org,target_class):\n",
        "  #print(target_class)\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z)).mean())\n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][0][target_class])\n",
        "    #print(Classifier_model(G(z))[0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    loss = -loss1 - loss2 - loss3\n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"loss2\" + \" \" + str(loss2))\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    if(i==10000):\n",
        "      break\n",
        "  \n",
        "  return z, loss1, loss2, loss3\n",
        "\n",
        "\n",
        "torch.manual_seed(100)\n",
        "z= torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "target_class = 1\n",
        "z_cf,loss1, loss2, loss3 = counterfactual_cosine(z,z_org,target_class)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z_cf)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALKaTtg0Jagc"
      },
      "source": [
        "# Save the latent cfs for all the digits from 0-9\n",
        "latent_cfs = []\n",
        "for cnt in range(0,10,1):\n",
        "  torch.manual_seed(100)\n",
        "  z= torch.randn(1, 100,1,1).to(device)\n",
        "  step_size = 0.01\n",
        "\n",
        "  target_class = cnt\n",
        "  z_cf,loss1, loss2, loss3 = counterfactual_cosine(z,z_org,target_class)\n",
        "  latent_cfs.append(z_cf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXNQzeiwFHNk"
      },
      "source": [
        "G_img= G(latent_cfs[0])\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "7Y5tMQbn2eTP",
        "outputId": "d29c6afe-c0cf-4fd3-ce07-0dda7f0232e8"
      },
      "source": [
        "# Check how the original one looks\n",
        "G_img= G(z_org)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa886641050>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7AzwDhvuHX-"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIyWCd2ZuL0f"
      },
      "source": [
        "Idea: Use the metric **Pred - argmax2nd highest** to identify the borderline semi factual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "j65ZpSd4uGZg",
        "outputId": "144ef507-28aa-475e-9a80-81704a936457"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "\n",
        "def semifactuals(z_org,z_semi,given_class):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z_semi.requires_grad = True\n",
        "    #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension euclidean\n",
        "    \n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "    loss2 = torch.log(Classifier_model(G(z_semi))[0][given_class]) # ensure the class is maintained \n",
        "    temp = Classifier_model(G(z_semi))[0]\n",
        "    loss3 = torch.log(1.000001 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "    loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "    \n",
        "    \n",
        "  #loss = loss1 - loss2 - 5*loss3 # without PR\n",
        "    loss = -loss1 - loss2 - loss3 - loss4 #with PR\n",
        "    z_semi.grad = None\n",
        "    loss.backward()\n",
        "    z_semi.requires_grad = False\n",
        "    z_semi = z_semi - z_semi.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  \n",
        "    if (i==1000):\n",
        "      break\n",
        "\n",
        "  return z_semi\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "# Run the function from here\n",
        "given_class = 0\n",
        "\n",
        "\n",
        "z_semi = semifactuals(z_semi,z_org,given_class)\n",
        "\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor([14.4118], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor([14.1344], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor([14.4118], grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa881853190>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YJZyOUDu5At",
        "outputId": "057465dc-fe23-4851-88c9-7cc5ee20284f"
      },
      "source": [
        "Classifier_model(G(z_semi))[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 6.8261e-16, 2.3747e-12, 1.5510e-16, 7.7738e-18, 1.2519e-17,\n",
              "        2.1426e-13, 8.3341e-14, 2.0627e-17, 7.0864e-16],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grw0cPaQvSy6",
        "outputId": "ce653775-fb14-4a99-dea6-ecafc185eb64"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_semi))[0],2)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([1.0000e+00, 2.3747e-12], grad_fn=<TopkBackward>), indices=tensor([0, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVyvVi0qvarn",
        "outputId": "9ce753d3-cac1-47c9-8800-873f264126e9"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_org))[0],2)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([1.0000e+00, 6.3456e-13], grad_fn=<TopkBackward>), indices=tensor([0, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Te8MATiweMU",
        "outputId": "bfa51e4b-4e70-4b4c-e0e4-45595a57c3cf"
      },
      "source": [
        "temp =   Classifier_model(G(z_semi))[0]\n",
        "torch.log(1.000001 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]]))\n",
        "1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1000, grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGDsY2j4NfG"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual provided a target class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "KW5JYRmG4ONB",
        "outputId": "47169580-3226-4b9d-ecd3-30d45b0ae511"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "import torch\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "#using latent dimension distance\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "given_class = 0\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "while (avg_loss_old>=avg_loss):\n",
        "  z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "  loss2 = torch.log(Classifier_model(G(z_semi))[0][0][given_class]) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "  loss3 = torch.log(1.0001 - torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))) # difference between target class and img class\n",
        "  loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = -loss1 - loss2 - loss3 -loss4 # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  z_semi.grad = None\n",
        "  loss.backward()\n",
        "  z_semi.requires_grad = False\n",
        "  z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5170ff78c746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_org\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_semi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# distance using cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_semi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgiven_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ensure the class is maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;31m#temp = Classifier_model(G(z_semi))[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m#loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Classifier_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-KVqNzvO4Vj2",
        "outputId": "33c8f8cf-64dd-4091-c27b-abe0aa4455f6"
      },
      "source": [
        "# Plotting the semi factual image according to the cf found by the latent dimension loss and target class provided\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "#print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa880dc15d0>"
            ]
          },
          "metadata": {},
          "execution_count": 232
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNEy930ofePD"
      },
      "source": [
        "## Using the standard losses for CF calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwi1rqET7vhE",
        "outputId": "cd38b57c-5333-4b95-c73e-014f788e3aef"
      },
      "source": [
        "#Tried using nn.losses\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "#Find the image closer to the original one but target prediction\n",
        "criterion1 = nn.MSELoss()\n",
        "nll = nn.NLLLoss()\n",
        "CE_loss = nn.CrossEntropyLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "optimizer = optim.SGD([z_semi], lr=0.01)\n",
        "\n",
        "#using latent dimension distance\n",
        "latent_z = []\n",
        "for cnt in range(1,10,1):\n",
        "  z_semi = z_org.to(device)\n",
        "\n",
        "  overall_loss = 0\n",
        "  i=1\n",
        "  while (i<=10000):\n",
        "    z_semi.requires_grad = True\n",
        "    #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "    \n",
        "    loss1 = criterion1(z_org,z_semi) # distance using MSE\n",
        "    loss2 = CE_loss(Classifier_model_logits(G(z_semi))[1],torch.tensor([cnt]).to(device) ) # ensure the class is maintained \n",
        "    loss3 = l1_loss(D(G(z_semi)), torch.tensor([[1.0]]).to(device))\n",
        "\n",
        "       \n",
        "    #loss =  loss1 + loss2  # without PR\n",
        "    loss = loss1 + loss2 + 0.1*loss3 #with PR\n",
        "    \n",
        "    z_semi.grad = None\n",
        "    loss.backward(retain_graph=True)  \n",
        "    optimizer.step()  \n",
        "    z_semi.requires_grad = False\n",
        "    z_semi = z_semi - z_semi.grad * step_size\n",
        "    i = i+1\n",
        "    overall_loss= overall_loss + loss \n",
        "\n",
        "    if (i%500==0):\n",
        "           \n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      #print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  latent_z.append(z_semi)\n",
        "  "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(2.3126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.1393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(1.1101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.1373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.1357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(0.3128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.0077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.0074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.0072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.0071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.0070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.0068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.0068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.0067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.0067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.0065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(1.7429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.1419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.1381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.1350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(0.5069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.0194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.0182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.0173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.0168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.0164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.0162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.0160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.0158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.0156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.0155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.0153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.0151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.0150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.0150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.0149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(2.0943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.2858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.2621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.2531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.2353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.2178, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(0.8001, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1216, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.1202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.1192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.1184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1141, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(0.5715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.1376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.1352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.1335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.1324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.1316, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.1309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.1303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.1298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.1294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.1291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.1288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.1285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.1283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.1281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.1278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.1274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.1270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  500tensor(1.0099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1000tensor(0.1076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(0.0353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(0.0323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(0.0300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(0.0276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(0.0263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(0.0253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(0.0247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(0.0240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(0.0221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(0.0204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(0.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(0.0184, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxzWmAMrRPLP",
        "outputId": "1b850a2c-5770-422d-9afa-d6554eec132c"
      },
      "source": [
        "loss1"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1253, grad_fn=<MseLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UencMIl4AkeZ",
        "outputId": "87608f3d-4ed7-4ae0-e3de-a5c4e51f9f1e"
      },
      "source": [
        "CE_loss(Classifier_model_logits(G(z_semi))[1],torch.tensor([6]) )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7275, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NsIdOjD4zg5",
        "outputId": "b27e3edc-c6ac-4ef9-bb0f-b7191e5a87cd"
      },
      "source": [
        "Classifier_model_logits(G(z_semi))[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.7433e-07, 9.9904e-01, 2.4590e-07, 1.2178e-04, 2.9775e-04, 2.6639e-05,\n",
              "         1.0285e-07, 5.6574e-05, 2.2462e-04, 2.2825e-04]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUSfB-VW6CI8",
        "outputId": "432b3b94-f0ea-4ff1-d6a6-9e4468f977c6"
      },
      "source": [
        "loss2"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0075, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_liI5seW441",
        "outputId": "8fdaccab-f253-44c9-f2c0-0640ec33b2a3"
      },
      "source": [
        "loss3"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0019, grad_fn=<L1LossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kfSI7W_C_MT_",
        "outputId": "01cfcb2f-3f2e-4ae8-ae6f-e726be3cedc6"
      },
      "source": [
        "# Plotting the semi factual image according to the cf found by the latent dimension loss and target class provided\n",
        "G_img= G(latent_z[8])\n",
        "G_img = G_img.to('cpu')\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "#print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd63ba46a90>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMhCvRGtXjyl",
        "outputId": "346148b5-d0a8-4f77-900b-9737be5eb832"
      },
      "source": [
        "\n",
        "Classifier_model_logits(G(latent_z[1]))[0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.7616e-03, 3.0601e-05, 9.9420e-01, 2.9991e-03, 2.4383e-08, 2.4623e-07,\n",
              "         4.0001e-06, 5.8101e-06, 7.3204e-08, 2.2983e-07]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdwdnjL3HDIe",
        "outputId": "e782fd87-43b3-4428-b2af-0eea8e68c00a"
      },
      "source": [
        "# trying some different optimisation technique\n",
        "\n",
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "print(torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1])))\n",
        "while (torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))>0.1):\n",
        "  z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "  loss2 = torch.log(Classifier_model(G(z_semi))[0][0]) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "  loss3 = torch.log(1.0001 - torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))) # difference between target class and img class\n",
        "  loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = -loss1 - loss2 - loss3 -loss4 # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  z_semi.grad = None\n",
        "  loss.backward()\n",
        "  z_semi.requires_grad = False\n",
        "  z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.4309e-06, grad_fn=<AbsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFzrer3duXem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}