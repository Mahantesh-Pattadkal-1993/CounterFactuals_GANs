{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Counterfactual Computation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q7AzwDhvuHX-"
      ],
      "authorship_tag": "ABX9TyMWemMkHUGYcdOQgrbrg+h+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahantesh-Pattadkal-1993/CounterFactuals_GANs/blob/main/Counterfactual_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWIBBfXcP_kz",
        "outputId": "a29754f4-e2ae-4d4b-d92d-0e8d340c6fb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uma5W4TFQSwX"
      },
      "source": [
        "#Setting up the path for GAN Training\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Github/CounterFactuals_GANs/MNIST')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amOxIb6xQVbM",
        "outputId": "a5283820-934c-4f6d-de21-8a47d024c257"
      },
      "source": [
        "#Import the libraries \n",
        "import torch\n",
        "import torch.optim as opt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from GAN_Models import DC_Generator, DC_Discriminator, Net, CNN, Net_logits\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 2021\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbb40848e30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnkacTUZQu_",
        "outputId": "bb0840ae-b945-4844-c2e6-f00488a0d789"
      },
      "source": [
        "#Load the Generator  \n",
        "# using the 150 epochs dataset\n",
        "\n",
        "G = DC_Generator()\n",
        "D = DC_Discriminator()\n",
        "\n",
        "checkpoint = torch.load(\"Weights/G_checkpoint_latest_149.pth\", map_location=torch.device('cpu'))\n",
        "G.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "checkpoint = torch.load(\"Weights/D_checkpoint_latest_149.pth\", map_location=torch.device('cpu'))\n",
        "D.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxUuFeUMQdht",
        "outputId": "131bf78d-12bb-48e8-b135-3931756f485e"
      },
      "source": [
        "#Load the Classifier\n",
        "Classifier_model = Net()\n",
        "Classifier_checkpoint = torch.load(\"Weights/Classifier_CNN.pth\", map_location=torch.device('cpu'))\n",
        "Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model.eval()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QgUV9HIGHoI",
        "outputId": "af4db6c7-2ee0-4d85-f8cb-6172d8294d1e"
      },
      "source": [
        "#Load the Classifier Net_logits() and outputs :- softmax, logits\n",
        "Classifier_model_logits = Net_logits()\n",
        "Classifier_checkpoint = torch.load(\"Weights/Classifier_CNN_logits.pth\", map_location=torch.device('cpu'))\n",
        "Classifier_model_logits.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model_logits.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net_logits(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sur5CGLPYCj4",
        "outputId": "b1ae0602-e3e3-4b5b-dbef-3521ab7545bb"
      },
      "source": [
        "#Load Eoins Classifier\n",
        "Classifier_model_eoin = CNN()\n",
        "Classifier_checkpoint = torch.load(\"Weights/pytorch_cnn.pth\", map_location=torch.device('cpu'))\n",
        "#Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model_eoin.eval()\n",
        "\n",
        "# Gettin weird predictions, maybe the state dict is missing"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout2d(p=0.1, inplace=False)\n",
              "    (4): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Dropout2d(p=0.1, inplace=False)\n",
              "    (8): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Dropout2d(p=0.1, inplace=False)\n",
              "    (12): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): Dropout2d(p=0.2, inplace=False)\n",
              "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHnhp6XXRCby",
        "outputId": "0eebee88-7cc1-4f02-ecc8-986fa275408b"
      },
      "source": [
        "#Loading the data\n",
        "\n",
        "mb_size = 2\n",
        "\n",
        "transform = transforms.Compose(\n",
        "\t\t[transforms.ToTensor(),\n",
        "\t\t transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainData = torchvision.datasets.MNIST('./data/', download=True, transform=transform, train=True)\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, shuffle=False, batch_size=mb_size)\n",
        "\n",
        "dataIter = iter(trainLoader)\n",
        "\n",
        "imgs, labels = dataIter.next()\n",
        "print(imgs.shape)\n",
        "print(labels)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 28, 28])\n",
            "tensor([5, 0])\n",
            "tensor([[5.5763e-15, 1.1678e-13, 1.9366e-16, 3.5489e-03, 1.8869e-19, 9.9645e-01,\n",
            "         2.5875e-13, 6.4613e-16, 1.5177e-08, 6.3372e-10],\n",
            "        [1.0000e+00, 8.4316e-11, 1.7799e-08, 5.8022e-12, 1.0353e-11, 1.9533e-13,\n",
            "         2.2826e-09, 1.6658e-10, 1.5406e-11, 1.8834e-11]],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "J4QajikfSHfx",
        "outputId": "9215ba72-8326-45fc-d819-d75fe144a7c7"
      },
      "source": [
        "\n",
        "\n",
        "sample_image = imgs[1,:,:,:] #first image in this batch \n",
        "sample_image= sample_image.reshape([1,1,28,28])\n",
        "\n",
        "npimgs = sample_image[0].detach().numpy()\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')\n",
        "output, logits = Classifier_model_logits(sample_image)\n",
        "print(output)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0000e+00, 8.4316e-11, 1.7799e-08, 5.8022e-12, 1.0353e-11, 1.9533e-13,\n",
            "         2.2826e-09, 1.6658e-10, 1.5406e-11, 1.8834e-11]],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGVngIZliuGo",
        "outputId": "8ef95827-965d-4ac2-d031-a86150d97525"
      },
      "source": [
        "x,y = Classifier_model_eoin(imgs)\n",
        "m=nn.Softmax(dim=1)\n",
        "x_softmax = m(x)\n",
        "print(x_softmax)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1082, 0.0967, 0.0918, 0.0994, 0.1060, 0.1018, 0.1078, 0.0942, 0.0985,\n",
            "         0.0956],\n",
            "        [0.1082, 0.0967, 0.0918, 0.0994, 0.1060, 0.1017, 0.1078, 0.0942, 0.0985,\n",
            "         0.0956]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--otHGKdTbBh"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "kBQFg3WiaO4G",
        "outputId": "5179fde0-97a1-4e9e-a9c9-435b354aaabb"
      },
      "source": [
        "#find the z_org that respresents the latent representation of given image\n",
        "\n",
        "def latent_representation(z,sample_image,pred):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(sample_image-G(z))).sum() \n",
        "    loss2 = torch.square(torch.abs(Classifier_model_logits(G(z))[0] - Classifier_model_logits(sample_image)[0]).sum())\n",
        "    #loss2 = torch.square(torch.abs(Classifier_model_logits(G(z)) - Classifier_model_logits(sample_image)).sum()) # use when Net() model is used\n",
        "    \n",
        "\n",
        "    loss = loss1 + loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    \n",
        "    if(i==10000):\n",
        "      break\n",
        "\n",
        "  return z\n",
        "\n",
        "z_org = latent_representation(z,sample_image,4)\n",
        "\n",
        "G_img= G(z_org)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(216.0323, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(32.6574, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(216.0323, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(17.2728, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(32.6574, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(15.1841, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(17.2728, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(13.8927, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(15.1841, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(13.0699, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(13.8927, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(12.2972, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(13.0699, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(11.4474, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(12.2972, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(10.8372, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(11.4474, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(10.2656, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(10.8372, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(9.5355, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor(10.2656, grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(9.3095, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor(9.5355, grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(9.1630, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor(9.3095, grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(8.9924, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor(9.1630, grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(8.8377, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor(8.9924, grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(8.7466, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor(8.8377, grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(8.6194, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8500tensor(8.7466, grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(8.3958, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9000tensor(8.6194, grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(8.3063, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9500tensor(8.3958, grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(8.2256, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 10000tensor(8.3063, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbb38984310>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCFW5KPwnBn",
        "outputId": "5a15a789-f51e-4b5c-a0c4-c2b3919dea86"
      },
      "source": [
        "Classifier_model_logits(G(z))[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3555e-06, 9.3431e-12, 2.9211e-12, 1.9332e-12, 3.1462e-12, 5.2354e-05,\n",
              "         9.9993e-01, 1.9210e-13, 2.0184e-05, 2.9865e-13]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHzqEQW8qZUM"
      },
      "source": [
        "## Latent dimension loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "2FRhTyamqTNh",
        "outputId": "ff6e7abb-3a35-4e47-c94c-93dc95f033f1"
      },
      "source": [
        "#using latent dimension euclidean distance and PR loss\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() #latent dim euclidean loss\n",
        "    loss2 = torch.log(Classifier_model_logits(G(z))[0][0][target_class]) #as classifier gives 2 outputs we take 0th index\n",
        "    loss3 = torch.log(D(G(z))) #plausibility\n",
        "\n",
        "    loss = loss1 - loss2 \n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z, loss1, loss2, loss3\n",
        "\n",
        "target_class = 1\n",
        "z_eucli, loss1, loss2, loss3 = counterfactual_eluclidian(z,z_org,target_class)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z_eucli)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(44.3367, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(11.3798, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(44.3367, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(5.1295, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(11.3798, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(4.1300, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(5.1295, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(3.9595, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(4.1300, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(3.9356, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(3.9595, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(3.9402, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(3.9356, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbb386abb50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFnCruWDtsLu",
        "outputId": "2e2feda7-da48-45e2-d0f4-e3d8aa5b7b91"
      },
      "source": [
        "Classifier_model_logits(G(z_eucli))[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8972e-03, 4.9882e-01, 1.5683e-01, 6.7859e-02, 1.2414e-01, 2.1038e-05,\n",
              "         1.3386e-01, 1.0421e-02, 6.0656e-03, 8.5337e-05]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "0ejFwG9tsNvD",
        "outputId": "b13dbdcd-74fd-449c-fe0a-6e320ba7309b"
      },
      "source": [
        "#using latent dimension distance and without PR loss\n",
        "\n",
        "#This can be used as the baselines that do not talk about plausibility\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() \n",
        "    loss2 = torch.log(Classifier_model_logits(G(z))[0][0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    #loss = loss1 - 10*loss2 - loss3\n",
        "    loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z\n",
        "\n",
        "z = counterfactual_eluclidian(z,z_org,3)\n",
        "\n",
        "#Check if the image generated is close to the real image\n",
        "G_img= G(z)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(12.6871, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f03fab9b310>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCKydwVltbGi"
      },
      "source": [
        "#using latent dimension distance but cosine similarity\n",
        "import torch\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "def counterfactual_cosine(z,z_org,target_class):\n",
        "  #print(target_class)\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z)).mean())\n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][0][target_class])\n",
        "    #print(Classifier_model(G(z))[0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    loss = -loss1 - loss2 - loss3\n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"loss2\" + \" \" + str(loss2))\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    if(i==10000):\n",
        "      break\n",
        "  \n",
        "  return z, loss1, loss2, loss3\n",
        "\n",
        "\n",
        "torch.manual_seed(100)\n",
        "z= torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "target_class = 1\n",
        "z_cf,loss1, loss2, loss3 = counterfactual_cosine(z,z_org,target_class)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z_cf)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALKaTtg0Jagc"
      },
      "source": [
        "# Save the latent cfs for all the digits from 0-9\n",
        "latent_cfs = []\n",
        "for cnt in range(0,10,1):\n",
        "  torch.manual_seed(100)\n",
        "  z= torch.randn(1, 100,1,1).to(device)\n",
        "  step_size = 0.01\n",
        "\n",
        "  target_class = cnt\n",
        "  z_cf,loss1, loss2, loss3 = counterfactual_cosine(z,z_org,target_class)\n",
        "  latent_cfs.append(z_cf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXNQzeiwFHNk"
      },
      "source": [
        "G_img= G(latent_cfs[0])\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "7Y5tMQbn2eTP",
        "outputId": "d29c6afe-c0cf-4fd3-ce07-0dda7f0232e8"
      },
      "source": [
        "# Check how the original one looks\n",
        "G_img= G(z_org)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa886641050>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7AzwDhvuHX-"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIyWCd2ZuL0f"
      },
      "source": [
        "Idea: Use the metric **Pred - argmax2nd highest** to identify the borderline semi factual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "j65ZpSd4uGZg",
        "outputId": "144ef507-28aa-475e-9a80-81704a936457"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "\n",
        "def semifactuals(z_org,z_semi,given_class):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z_semi.requires_grad = True\n",
        "    #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension euclidean\n",
        "    \n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "    loss2 = torch.log(Classifier_model(G(z_semi))[0][given_class]) # ensure the class is maintained \n",
        "    temp = Classifier_model(G(z_semi))[0]\n",
        "    loss3 = torch.log(1.000001 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "    loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "    \n",
        "    \n",
        "  #loss = loss1 - loss2 - 5*loss3 # without PR\n",
        "    loss = -loss1 - loss2 - loss3 - loss4 #with PR\n",
        "    z_semi.grad = None\n",
        "    loss.backward()\n",
        "    z_semi.requires_grad = False\n",
        "    z_semi = z_semi - z_semi.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  \n",
        "    if (i==1000):\n",
        "      break\n",
        "\n",
        "  return z_semi\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "# Run the function from here\n",
        "given_class = 0\n",
        "\n",
        "\n",
        "z_semi = semifactuals(z_semi,z_org,given_class)\n",
        "\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor([14.4118], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor([14.1344], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor([14.4118], grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa881853190>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YJZyOUDu5At",
        "outputId": "057465dc-fe23-4851-88c9-7cc5ee20284f"
      },
      "source": [
        "Classifier_model(G(z_semi))[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 6.8261e-16, 2.3747e-12, 1.5510e-16, 7.7738e-18, 1.2519e-17,\n",
              "        2.1426e-13, 8.3341e-14, 2.0627e-17, 7.0864e-16],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grw0cPaQvSy6",
        "outputId": "ce653775-fb14-4a99-dea6-ecafc185eb64"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_semi))[0],2)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([1.0000e+00, 2.3747e-12], grad_fn=<TopkBackward>), indices=tensor([0, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVyvVi0qvarn",
        "outputId": "9ce753d3-cac1-47c9-8800-873f264126e9"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_org))[0],2)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([1.0000e+00, 6.3456e-13], grad_fn=<TopkBackward>), indices=tensor([0, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Te8MATiweMU",
        "outputId": "bfa51e4b-4e70-4b4c-e0e4-45595a57c3cf"
      },
      "source": [
        "temp =   Classifier_model(G(z_semi))[0]\n",
        "torch.log(1.000001 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]]))\n",
        "1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1000, grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGDsY2j4NfG"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual provided a target class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW5JYRmG4ONB"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "while (avg_loss_old>=avg_loss):\n",
        "  z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "  loss2 = torch.log(Classifier_model(G(z_semi))[0][0]) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "  loss3 = torch.log(1.0001 - torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))) # difference between target class and img class\n",
        "  loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = -loss1 - loss2 - loss3 -loss4 # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  z_semi.grad = None\n",
        "  loss.backward()\n",
        "  z_semi.requires_grad = False\n",
        "  z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-KVqNzvO4Vj2",
        "outputId": "33c8f8cf-64dd-4091-c27b-abe0aa4455f6"
      },
      "source": [
        "# Plotting the semi factual image according to the cf found by the latent dimension loss and target class provided\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "#print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa880dc15d0>"
            ]
          },
          "metadata": {},
          "execution_count": 232
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "Zwi1rqET7vhE",
        "outputId": "c443f4e5-e6fd-432b-da8b-2f42b6fbf9c0"
      },
      "source": [
        "#Tried using nn.losses\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "#Find the image closer to the original one but target prediction\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2= nn.MSELoss()\n",
        "nll = nn.NLLLoss()\n",
        "CE_loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([z_semi], lr=0.001)\n",
        "#using latent dimension distance\n",
        "\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "while (i<=5000):\n",
        "  #z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = criterion1(z_org,z_semi) # distance using MSE\n",
        "  loss2 = CE_loss(Classifier_model_logits(G(z_semi))[1],torch.tensor([1]) ) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = criterion(temp[torch.topk(temp,2).indices[0]] , temp[torch.topk(temp,2).indices[1]]) # diference between highest and argmax 2\n",
        "  #loss3 = torch.log(1.1 - torch.abs((Classifier_model(G(z_semi))[0][4] - Classifier_model(G(z_semi))[0][6]))) # difference between target class and img class\n",
        "  #loss4 = nll(D(G(z_semi)),torch.tensor([0])) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = loss1 + loss2   # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  #z_semi.grad = None\n",
        "  loss.backward(retain_graph=True)  \n",
        "  optimizer.step()  \n",
        "  #z_semi.requires_grad = False\n",
        "  #z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(26.1565, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(26.2089, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(26.1565, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(26.2089, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(26.2089, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(26.2089, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(26.2089, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-503b870e9a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_org\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_semi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# distance using MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier_model_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_semi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# ensure the class is maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0;31m#temp = Classifier_model(G(z_semi))[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m#loss3 = criterion(temp[torch.topk(temp,2).indices[0]] , temp[torch.topk(temp,2).indices[1]]) # diference between highest and argmax 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/Github/CounterFactuals_GANs/MNIST/GAN_Models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UencMIl4AkeZ",
        "outputId": "9cb2e773-f9bd-48a8-890c-0094bfac8085"
      },
      "source": [
        "Classifier_model(G(z_semi))[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7.4310e-06, 2.0573e-11, 4.5032e-11, 1.6493e-13, 3.5028e-13, 5.5022e-04,\n",
              "        9.9944e-01, 3.1245e-15, 1.9041e-06, 1.0009e-12],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NsIdOjD4zg5",
        "outputId": "38138d92-7a4b-404c-8d36-d10cc730db7c"
      },
      "source": [
        "CE_loss(torch.tensor([[0.5]]), torch.tensor([0]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUSfB-VW6CI8",
        "outputId": "d7f08c4e-7730-412c-879c-0c787e792ef6"
      },
      "source": [
        "loss1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8050)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kfSI7W_C_MT_",
        "outputId": "9d2d99d1-48c4-4643-f2a4-58671ec0d331"
      },
      "source": [
        "# Plotting the semi factual image according to the cf found by the latent dimension loss and target class provided\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "#print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbb388d7890>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOk0lEQVR4nO3db4xV9Z3H8c8XHKJCo7DIOJni0jYaRKPSDGbjn01XoFEfCH1gA4mNRpLpg5pUs4nF+qCTbEzM7rL7sHGwWjRdSVW0CE2pkrpuYwRHcPm7rX+CAhkgOBJERBzmuw/msDvqnN8Z7j33njvzfb+Syb33fOfc++VmPpxz7++c8zN3F4CJb1LVDQBoDsIOBEHYgSAIOxAEYQeCOK+ZL2ZmfPUPNJi722jL69qym9mtZvYXM3vXzFbW81wAGstqHWc3s8mS/ippsaQDkt6UtNzd9yTWYcsONFgjtuzXS3rX3d9399OS1kpaUsfzAWigesLeKWn/iMcHsmVfYmbdZtZnZn11vBaAOjX8Czp375XUK7EbD1Spni37QUmzRzz+ZrYMQAuqJ+xvSrrczL5lZlMkLZO0vpy2AJSt5t14dx80s/skbZI0WdIT7r67tM4AlKrmobeaXozP7EDDNeSgGgDjB2EHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0E09VLSGF1bW1uy/txzzyXr27Zty609++yzyXX37Mm9PigmGLbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEV5dtgilTpiTrGzduTNYXLlyYrA8ODubWduzYkVz3wQcfTNZTY/iSdOzYsWQdzcfVZYHgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZm2DLli3JeldXV7I+aVL6/+SPPvoot7Zo0aLkulu3bk3Wi8bRZ82alayj+fLG2eu6eIWZ7ZP0iaQzkgbdPf1XC6AyZVyp5h/c/WgJzwOggfjMDgRRb9hd0h/N7C0z6x7tF8ys28z6zKyvztcCUId6d+NvcveDZjZL0stm9j/u/trIX3D3Xkm9Utwv6IBWUNeW3d0PZrdHJL0g6foymgJQvprDbmZTzewbZ+9L+r6kXWU1BqBc9ezGt0t6wczOPs9/uPsfSulqnDn//POT9WuuuSZZLxpHHxoaStYfeeSR3NqVV16ZXHfy5MnJelFvHR0dyXp/f3+yjuapOezu/r6ka0vsBUADMfQGBEHYgSAIOxAEYQeCIOxAEJziWoK77rorWX/66afrev7PP/88Wb/44otza6dOnarrtTH+cClpIDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiijAtOhnDDDTfk1h577LGGvvbatWuTdcbSMRZs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCM5nH6PUOeMDAwPJdbPLbecqOl+9aErnXbu4XD/+H+ezA8ERdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLOP0ZQpU3JrH3/8cXLdCy+8MFk/duxYsn7JJZck64ODg8l6IxVN+dzW1pZbKzq+oEgz/3bHk5rH2c3sCTM7Yma7RiybYWYvm9k72e30MpsFUL6x7Mb/WtKtX1m2UtJmd79c0ubsMYAWVhh2d39N0lePB10iaU12f42kpSX3BaBktV6Drt3d+7P7hyS15/2imXVL6q7xdQCUpO4LTrq7p754c/deSb3S+P6CDhjvah16O2xmHZKU3R4pryUAjVBr2NdLuju7f7ek35XTDoBGKdyNN7NnJH1P0kwzOyDpF5IelfRbM1sh6QNJP2xkk63g9OnTubX9+/cn173iiiuS9dtuuy1Zr2cc/eqrr07WN23alKynji+QpGnTpiXrkyblb0+GhoaS6z7++OPJ+gMPPJCsV3n8QSsqDLu7L88pLSy5FwANxOGyQBCEHQiCsANBEHYgCMIOBMEpriUoGkIqupT0U089layvWLEiWZ87d25ubfv27cl1i05RLVI0vJX6txf97RW9rw8//HCyvmrVqmR9ouJS0kBwhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsJaj3Pezp6UnW169fn6y/8cYbubWiU1RTp+5K0kUXXZSsnzp1KllPuffee5P13t7eZP2zzz5L1lO9F43hj2eMswPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzl+DTTz9N1i+44IJk/Y477kjWL7300mR99erVubWi8eQbb7wxWU+N4der6Dz/48ePJ+tFl7FesGBBbq2vry+57njGODsQHGEHgiDsQBCEHQiCsANBEHYgCMIOBME4ewleffXVZP3mm29O1k+ePJmsnzhxIlmfOXNmbu3OO+9Mrvviiy8m61Xq7OxM1g8cOJCsDwwM1Pzc9ZynX7Wax9nN7AkzO2Jmu0Ys6zGzg2b2dvZze5nNAijfWHbjfy3p1lGW/7u7X5f9/L7ctgCUrTDs7v6apPz9IQDjQj1f0N1nZjuy3fzpeb9kZt1m1mdmE/dgZGAcqDXsv5T0HUnXSeqXlDuDnrv3unuXu3fV+FoASlBT2N39sLufcfchSaslXV9uWwDKVlPYzaxjxMMfSNqV97sAWkPhOLuZPSPpe5JmSjos6RfZ4+skuaR9kn7s7v2FLzZBx9k//PDDZH327Nl1PX/R9dGXLVuWWyu65nwrKzrf/cyZMzWvv3///uS6l112WbLeyvLG2c8bw4rLR1n8q7o7AtBUHC4LBEHYgSAIOxAEYQeCIOxAEJziWoL58+cn60WXLZ40Kf1/7uDgYLLe1taWrE9UX3zxRbJ+3nn5g01Hjx5Nrtve3p6st/KUz1xKGgiOsANBEHYgCMIOBEHYgSAIOxAEYQeCKDzrDcV27tyZrG/evDlZX7x4cbL+yiuvnHNPE8HUqVOT9dQ4epFVq3IvriSptcfRa8WWHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJy9BEXnm2/cuDFZX7RoUbL+0EMPnXNPrSJ1rv7cuXOT627fvr2u1z5+/Hhu7cknn6zruccjtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7E2wZMmSZL1oauKenp5kfenSpefaUmmKrnmfumb+tddeW9dr7969O1lfsGBBbq1oGuyJqHDLbmazzexPZrbHzHab2U+z5TPM7GUzeye7nd74dgHUaiy78YOS/tHd50n6O0k/MbN5klZK2uzul0vanD0G0KIKw+7u/e6+Lbv/iaS9kjolLZG0Jvu1NZKq25cEUOicPrOb2RxJ8yVtkdTu7v1Z6ZCkUSfHMrNuSd21twigDGP+Nt7Mpkl6XtL97v6lMwx8eHbIUSdtdPded+9y9666OgVQlzGF3czaNBz037j7umzxYTPryOodko40pkUAZSicstmGx4XWSBpw9/tHLP8XSR+5+6NmtlLSDHd/sOC5JuSUzUWKTmF96aWXkvWi4a0NGzbk1tatW5dbk6RbbrklWZ81a1ayXjR81tnZmVt77733kuvec889yfrrr7+erEeVN2XzWD6z3yjpR5J2mtnb2bKfS3pU0m/NbIWkDyT9sIxGATRGYdjd/c+S8o76WFhuOwAahcNlgSAIOxAEYQeCIOxAEIQdCKJwnL3UFws6zt7W1pasb926NVkvGssuOkW2kfbt25esX3XVVbm1kydPltwNpPxxdrbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wtoOh89Xnz5iXrc+bMya3t3bs3ue6hQ4eS9aJLLg8NDSXraD7G2YHgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZgQmGcXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCKIw7GY228z+ZGZ7zGy3mf00W95jZgfN7O3s5/bGtwugVoUH1ZhZh6QOd99mZt+Q9JakpRqej/2Eu//rmF+Mg2qAhss7qGYs87P3S+rP7n9iZnsldZbbHoBGO6fP7GY2R9J8SVuyRfeZ2Q4ze8LMpues021mfWbWV1enAOoy5mPjzWyapP+U9Ii7rzOzdklHJbmkf9Lwrv69Bc/BbjzQYHm78WMKu5m1SdogaZO7/9so9TmSNrj71QXPQ9iBBqv5RBgbniL0V5L2jgx69sXdWT+QtKveJgE0zli+jb9J0n9J2inp7HWDfy5puaTrNLwbv0/Sj7Mv81LPxZYdaLC6duPLQtiBxuN8diA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCFF5ws2VFJH4x4PDNb1opatbdW7Uuit1qV2dvf5hWaej77117crM/duyprIKFVe2vVviR6q1WzemM3HgiCsANBVB323opfP6VVe2vVviR6q1VTeqv0MzuA5ql6yw6gSQg7EEQlYTezW83sL2b2rpmtrKKHPGa2z8x2ZtNQVzo/XTaH3hEz2zVi2Qwze9nM3sluR51jr6LeWmIa78Q045W+d1VPf970z+xmNlnSXyUtlnRA0puSlrv7nqY2ksPM9knqcvfKD8Aws7+XdELSU2en1jKzf5Y04O6PZv9RTnf3n7VIbz06x2m8G9Rb3jTj96jC967M6c9rUcWW/XpJ77r7++5+WtJaSUsq6KPluftrkga+sniJpDXZ/TUa/mNpupzeWoK797v7tuz+J5LOTjNe6XuX6Kspqgh7p6T9Ix4fUGvN9+6S/mhmb5lZd9XNjKJ9xDRbhyS1V9nMKAqn8W6mr0wz3jLvXS3Tn9eLL+i+7iZ3/66k2yT9JNtdbUk+/BmslcZOfynpOxqeA7Bf0qoqm8mmGX9e0v3ufnxkrcr3bpS+mvK+VRH2g5Jmj3j8zWxZS3D3g9ntEUkvaPhjRys5fHYG3ez2SMX9/B93P+zuZ9x9SNJqVfjeZdOMPy/pN+6+Lltc+Xs3Wl/Net+qCPubki43s2+Z2RRJyyStr6CPrzGzqdkXJzKzqZK+r9abinq9pLuz+3dL+l2FvXxJq0zjnTfNuCp+7yqf/tzdm/4j6XYNfyP/nqSHq+ghp69vS/rv7Gd31b1JekbDu3VfaPi7jRWS/kbSZknvSHpF0owW6u1pDU/tvUPDweqoqLebNLyLvkPS29nP7VW/d4m+mvK+cbgsEARf0AFBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEP8LEH/ap18kYbMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdwdnjL3HDIe",
        "outputId": "e782fd87-43b3-4428-b2af-0eea8e68c00a"
      },
      "source": [
        "# trying some different optimisation technique\n",
        "\n",
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "print(torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1])))\n",
        "while (torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))>0.1):\n",
        "  z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "  loss2 = torch.log(Classifier_model(G(z_semi))[0][0]) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "  loss3 = torch.log(1.0001 - torch.abs((Classifier_model(G(z_semi))[0][0] - Classifier_model(G(z_semi))[0][1]))) # difference between target class and img class\n",
        "  loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = -loss1 - loss2 - loss3 -loss4 # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  z_semi.grad = None\n",
        "  loss.backward()\n",
        "  z_semi.requires_grad = False\n",
        "  z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.4309e-06, grad_fn=<AbsBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFzrer3duXem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}