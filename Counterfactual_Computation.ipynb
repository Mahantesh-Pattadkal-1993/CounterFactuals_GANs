{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Counterfactual Computation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYnznkkrGFEbPppXJ3jVZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahantesh-Pattadkal-1993/CounterFactuals_GANs/blob/main/Counterfactual_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWIBBfXcP_kz",
        "outputId": "b738389c-9b5a-476e-f987-d4ed090a369f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uma5W4TFQSwX"
      },
      "source": [
        "#Setting up the path for GAN Training\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Github/CounterFactuals_GANs/MNIST')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amOxIb6xQVbM",
        "outputId": "db65ba8c-3229-48a4-add7-eef307f207bb"
      },
      "source": [
        "#Import the libraries \n",
        "import torch\n",
        "import torch.optim as opt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from GAN_Models import DC_Generator, DC_Discriminator, Net, CNN\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 2021\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9a25d1cf90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnkacTUZQu_",
        "outputId": "1ec14c9e-3ecc-4078-ae4d-d8061fa97825"
      },
      "source": [
        "#Load the Generator  \n",
        "G = DC_Generator(1,100,28)\n",
        "D = DC_Discriminator(nc=1,ndf =28)\n",
        "\n",
        "checkpoint = torch.load(\"Weights/G_checkpoint_latest_99.pth\", map_location=torch.device('cpu'))\n",
        "G.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "checkpoint = torch.load(\"Weights/D_checkpoint_latest_99.pth\", map_location=torch.device('cpu'))\n",
        "D.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxUuFeUMQdht",
        "outputId": "41fba1db-4964-492d-e0ed-830063295b63"
      },
      "source": [
        "#Load the Classifier\n",
        "Classifier_model = Net()\n",
        "Classifier_checkpoint = torch.load(\"Weights/Classifier.pth\", map_location=torch.device('cpu'))\n",
        "Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sur5CGLPYCj4"
      },
      "source": [
        "#Load Eoins Classifier\n",
        "Classifier_model_eoin = CNN()\n",
        "Classifier_checkpoint = torch.load(\"Weights/pytorch_cnn.pth\", map_location=torch.device('cpu'))\n",
        "#Classifier_model.load_state_dict(Classifier_checkpoint['state_dict'])\n",
        "Classifier_model_eoin.eval()\n",
        "\n",
        "# Gettin weird predictions, maybe the state dict is missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHnhp6XXRCby",
        "outputId": "a5ed1ee1-0294-4268-9cd1-a7a7806df22a"
      },
      "source": [
        "#Loading the data\n",
        "\n",
        "mb_size = 64\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "trainData = torchvision.datasets.MNIST('./data/', download=True, transform=transform, train=True)\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainData, shuffle=True, batch_size=mb_size)\n",
        "\n",
        "dataIter = iter(trainLoader)\n",
        "\n",
        "imgs, labels = dataIter.next()\n",
        "print(imgs.shape)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "J4QajikfSHfx",
        "outputId": "2cbbf9ac-f2dc-4bcb-ad3d-85e817916d39"
      },
      "source": [
        "dataIter = iter(trainLoader)\n",
        "\n",
        "imgs, labels = dataIter.next()\n",
        "sample_image = imgs[0,:,:,:] #first image in this batch \n",
        "sample_image= sample_image.reshape([1,1,28,28])\n",
        "\n",
        "npimgs = sample_image[0].detach().numpy()\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a22ab1990>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--otHGKdTbBh"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "kBQFg3WiaO4G",
        "outputId": "9cd2073a-d9ae-4f08-b181-0cb13e5299ec"
      },
      "source": [
        "#find the z_org that respresents the latent representation of given image\n",
        "\n",
        "def latent_representation(z,sample_image,pred):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(sample_image-G(z))).sum() \n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][pred])\n",
        "\n",
        "    #loss = loss1 - 10*loss2 - 10*loss3\n",
        "    loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    \n",
        "    if(i==10000):\n",
        "      break\n",
        "\n",
        "  return z\n",
        "\n",
        "z_org = latent_representation(z,sample_image,4)\n",
        "\n",
        "G_img= G(z_org)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(49.5241, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(37.5040, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(49.5241, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(35.6153, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(37.5040, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(33.7706, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(35.6153, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(31.7434, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(33.7706, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(29.4205, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(31.7434, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(28.6807, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(29.4205, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(28.0653, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(28.6807, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(27.4834, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(28.0653, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(26.9838, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(27.4834, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor(26.6525, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor(26.9838, grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor(26.4104, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor(26.6525, grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor(26.1981, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor(26.4104, grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor(26.0343, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor(26.1981, grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor(25.8755, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor(26.0343, grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor(25.6708, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor(25.8755, grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor(25.1266, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8500tensor(25.6708, grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor(24.9429, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9000tensor(25.1266, grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor(24.8375, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9500tensor(24.9429, grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor(24.6938, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 10000tensor(24.8375, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a1d3bf0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCFW5KPwnBn",
        "outputId": "1ee5f536-9364-4ddf-8b62-8dc9409fecab"
      },
      "source": [
        "Classifier_model(G(z_org))[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5.8124e-20, 1.4615e-22, 1.4803e-12, 4.5612e-20, 1.0000e+00, 4.2899e-16,\n",
              "        1.6857e-13, 5.9303e-14, 6.6829e-20, 3.1299e-06],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHzqEQW8qZUM"
      },
      "source": [
        "## Latent dimension loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "2FRhTyamqTNh",
        "outputId": "e3c24350-f855-4cd1-fd1f-67e20d1d7ced"
      },
      "source": [
        "#using latent dimension distance and PR loss\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() \n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    loss = loss1 - 10*loss2 - loss3\n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z\n",
        "\n",
        "\n",
        "z = counterfactual_eluclidian(z,z_org,5)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor([28.9334], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor([3.8907], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor([28.9334], grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor([1.4119], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor([3.8907], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor([0.9583], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor([1.4119], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor([0.8945], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor([0.9583], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor([0.8567], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor([0.8945], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor([0.8431], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor([0.8567], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor([0.8391], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor([0.8431], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor([0.8023], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor([0.8391], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor([0.5365], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor([0.8023], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor([0.4963], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor([0.5365], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor([0.4911], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor([0.4963], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor([0.4903], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor([0.4911], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor([0.4679], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor([0.4903], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor([0.4595], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor([0.4679], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor([0.4595], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8500tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9000tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9500tensor([0.4589], grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a1d55c810>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFnCruWDtsLu",
        "outputId": "39140773-70c2-4c4f-babb-e6c5c61b0881"
      },
      "source": [
        "Classifier_model(G(z))[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.1863e-10, 1.1846e-06, 9.9174e-01, 2.1092e-08, 3.0014e-04, 8.2560e-08,\n",
              "        1.4449e-10, 5.4091e-09, 1.9396e-03, 6.0206e-03],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "0ejFwG9tsNvD",
        "outputId": "b13dbdcd-74fd-449c-fe0a-6e320ba7309b"
      },
      "source": [
        "#using latent dimension distance and without PR loss\n",
        "\n",
        "#This can be used as the baselines that do not talk about plausibility\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def counterfactual_eluclidian(z,z_org,target_class):\n",
        "\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 = torch.square(torch.abs(z_org-z)).sum() \n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    #loss = loss1 - 10*loss2 - loss3\n",
        "    loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  return z\n",
        "\n",
        "z = counterfactual_eluclidian(z,z_org,3)\n",
        "\n",
        "#Check if the image generated is close to the real image\n",
        "G_img= G(z)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor(48.3900, grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor(16.9182, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor(13.0249, grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor(12.7657, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor(12.6995, grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor(12.6908, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor(12.6878, grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor(12.6876, grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor(12.6871, grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor(12.6864, grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f03fab9b310>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "pCKydwVltbGi",
        "outputId": "b6ed1aed-0e19-4e3d-8007-759ecb46956e"
      },
      "source": [
        "#using latent dimension distance but cosine similarity\n",
        "import torch\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "def counterfactual_cosine(z,z_org,target_class):\n",
        "  #print(target_class)\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z.requires_grad = True\n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z)).mean())\n",
        "    loss2 = torch.log(Classifier_model(G(z))[0][target_class])\n",
        "    #print(Classifier_model(G(z))[0][target_class])\n",
        "    loss3 = torch.log(D(G(z)))\n",
        "\n",
        "    loss = -loss1 - loss2 - loss3\n",
        "    #loss = loss1 - 10*loss2\n",
        "\n",
        "    z.grad = None\n",
        "    loss.backward()\n",
        "    z.requires_grad = False\n",
        "    z = z - z.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "    if(i==10000):\n",
        "      break\n",
        "  return z\n",
        "\n",
        "\n",
        "torch.manual_seed(100)\n",
        "z = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.01\n",
        "\n",
        "\n",
        "z = counterfactual_cosine(z,z_org,0)\n",
        "#Check if the image generated is close to the real image\n",
        "\n",
        "G_img= G(z)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor([2.0529], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor([-0.0304], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor([2.0529], grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor([-0.0327], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor([-0.0304], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor([-0.0345], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor([-0.0327], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor([-0.0362], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor([-0.0345], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor([-0.0377], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor([-0.0362], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor([-0.0391], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor([-0.0377], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor([-0.0403], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor([-0.0391], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor([-0.0413], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor([-0.0403], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor([-0.0422], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor([-0.0413], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor([-0.0429], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor([-0.0422], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor([-0.0437], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor([-0.0429], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor([-0.0444], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor([-0.0437], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor([-0.0452], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor([-0.0444], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor([-0.0459], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor([-0.0452], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor([-0.0465], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor([-0.0459], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor([-0.0470], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8500tensor([-0.0465], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor([-0.0474], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9000tensor([-0.0470], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor([-0.0478], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9500tensor([-0.0474], grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor([-0.0482], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 10000tensor([-0.0478], grad_fn=<DivBackward0>)\n",
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f03fa7ba1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 267
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrFSX3BIDRdr",
        "outputId": "7b339be8-ae02-4478-eea9-961b7a1ebb9c"
      },
      "source": [
        "Classifier_model(G(z))[0]"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4663e-10, 5.1856e-08, 4.4245e-10, 5.4238e-10, 3.2776e-05, 1.8631e-04,\n",
              "        5.8319e-09, 3.4236e-04, 9.9791e-01, 1.5309e-03],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KZ7HdRgDtEA",
        "outputId": "82e04fc0-cf96-493b-85f6-0319cc2d3b32"
      },
      "source": [
        "D(G(z))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9997], grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGKLCxij9k4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jne0Os8N55iQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7AzwDhvuHX-"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIyWCd2ZuL0f"
      },
      "source": [
        "Idea: Use the metric **Pred - argmax2nd highest** to identify the borderline semi factual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j65ZpSd4uGZg"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "\n",
        "def semifactuals(z_org,z_semi,given_class):\n",
        "  overall_loss = 0\n",
        "  loss = 0\n",
        "  avg_loss = 1000\n",
        "  avg_loss_old = 1000\n",
        "  i=1\n",
        "  while (avg_loss_old>=avg_loss):\n",
        "    z_semi.requires_grad = True\n",
        "    #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension euclidean\n",
        "    \n",
        "    loss1 =  torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "    loss2 = torch.log(Classifier_model(G(z_semi))[0][given_class]) # ensure the class is maintained \n",
        "    temp = Classifier_model(G(z_semi))[0]\n",
        "    loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "    loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "    \n",
        "    \n",
        "  #loss = loss1 - loss2 - 5*loss3 # without PR\n",
        "    loss = -loss1 - loss2 - loss3 - loss4 #with PR\n",
        "    z_semi.grad = None\n",
        "    loss.backward()\n",
        "    z_semi.requires_grad = False\n",
        "    z_semi = z_semi - z_semi.grad * step_size\n",
        "    i = i+1\n",
        "\n",
        "    overall_loss= overall_loss + loss\n",
        "    if (i%500==0):\n",
        "      #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "      avg_loss_old = avg_loss\n",
        "      avg_loss = overall_loss/500.0\n",
        "      print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "      print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "      overall_loss = 0 \n",
        "  \n",
        "  if (i==10000):\n",
        "    break\n",
        "\n",
        "  return z_semi\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "# Run the function from here\n",
        "given_class = 4\n",
        "\n",
        "\n",
        "z_semi = semifactuals(z_semi,z_org,given_class)\n",
        "\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YJZyOUDu5At",
        "outputId": "1094ddc7-b64a-444f-ac75-de39bbdf5d4f"
      },
      "source": [
        "Classifier_model(G(z_semi))[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.5007e-18, 3.6975e-24, 5.3301e-15, 5.0015e-22, 4.2488e-01, 5.9710e-10,\n",
              "        7.9007e-18, 4.6906e-13, 1.2912e-17, 5.7512e-01],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grw0cPaQvSy6",
        "outputId": "d9dcd152-da10-49d7-ba5f-7e29692281d5"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_semi))[0],2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([0.5236, 0.4764], grad_fn=<TopkBackward>), indices=tensor([4, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVyvVi0qvarn",
        "outputId": "c23e226b-76ed-4e40-eaca-0c13fa35ced8"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_org))[0],2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([1.0000e+00, 3.1299e-06], grad_fn=<TopkBackward>), indices=tensor([4, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Te8MATiweMU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGDsY2j4NfG"
      },
      "source": [
        "###Find the image which is on a borderline i.e. semi-factual provided a target class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KW5JYRmG4ONB",
        "outputId": "01226966-61a8-4a75-dfea-7a9916d43f2e"
      },
      "source": [
        "#Find the image distant from the original one but still has the same class perdiction\n",
        "\n",
        "#using latent dimension distance\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(100)\n",
        "z_semi = torch.randn(1, 100,1,1).to(device)\n",
        "step_size = 0.001\n",
        "\n",
        "overall_loss = 0\n",
        "loss = 0\n",
        "avg_loss = 1000\n",
        "avg_loss_old = 1000\n",
        "i=1\n",
        "while (avg_loss_old>=avg_loss):\n",
        "  z_semi.requires_grad = True\n",
        "  #loss1 = torch.square(torch.abs(z_org-z_semi)).sum() # distance in latent dimension\n",
        "  \n",
        "  loss1 = torch.log(0.1+torch.abs(cos(z_org,z_semi)).mean()) # distance using cosine similarity\n",
        "  loss2 = torch.log(Classifier_model(G(z_semi))[0][4]) # ensure the class is maintained \n",
        "  #temp = Classifier_model(G(z_semi))[0]\n",
        "  #loss3 = torch.log(1.1 - (temp[torch.topk(temp,2).indices[0]] - temp[torch.topk(temp,2).indices[1]])) # diference between highest and argmax 2\n",
        "  loss3 = torch.log(1.1 - torch.abs((Classifier_model(G(z_semi))[0][4] - Classifier_model(G(z_semi))[0][6]))) # difference between target class and img class\n",
        "  loss4 = torch.log(D(G(z_semi))) # Plausibility\n",
        "\n",
        "  \n",
        "  \n",
        "  loss = -loss1 - loss2 - loss3 -loss4 # without PR\n",
        "  #loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\n",
        "  z_semi.grad = None\n",
        "  loss.backward()\n",
        "  z_semi.requires_grad = False\n",
        "  z_semi = z_semi - z_semi.grad * step_size\n",
        "  i = i+1\n",
        "\n",
        "  overall_loss= overall_loss + loss\n",
        "  if (i%500==0):\n",
        "    #print(\"loss \" + \"step \" + str(i) + str (loss))\n",
        "    avg_loss_old = avg_loss\n",
        "    avg_loss = overall_loss/500.0\n",
        "    print(\"avg_loss \" + \"step \" + \" \" +str(i) + str (avg_loss))\n",
        "    print(\"avg_loss_old \" + \"step \" + str(i) + str (avg_loss_old))\n",
        "    overall_loss = 0 \n",
        " "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg_loss step  500tensor([20.6390], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5001000\n",
            "avg_loss step  1000tensor([13.3473], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1000tensor([20.6390], grad_fn=<DivBackward0>)\n",
            "avg_loss step  1500tensor([4.3748], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 1500tensor([13.3473], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2000tensor([1.3290], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2000tensor([4.3748], grad_fn=<DivBackward0>)\n",
            "avg_loss step  2500tensor([1.3171], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 2500tensor([1.3290], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3000tensor([1.3115], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3000tensor([1.3171], grad_fn=<DivBackward0>)\n",
            "avg_loss step  3500tensor([1.3066], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 3500tensor([1.3115], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4000tensor([1.3037], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4000tensor([1.3066], grad_fn=<DivBackward0>)\n",
            "avg_loss step  4500tensor([1.3015], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 4500tensor([1.3037], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5000tensor([1.2994], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5000tensor([1.3015], grad_fn=<DivBackward0>)\n",
            "avg_loss step  5500tensor([1.2973], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 5500tensor([1.2994], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6000tensor([1.2954], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6000tensor([1.2973], grad_fn=<DivBackward0>)\n",
            "avg_loss step  6500tensor([1.2936], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 6500tensor([1.2954], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7000tensor([1.2920], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7000tensor([1.2936], grad_fn=<DivBackward0>)\n",
            "avg_loss step  7500tensor([1.2903], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 7500tensor([1.2920], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8000tensor([1.2887], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8000tensor([1.2903], grad_fn=<DivBackward0>)\n",
            "avg_loss step  8500tensor([1.2869], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 8500tensor([1.2887], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9000tensor([1.2854], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9000tensor([1.2869], grad_fn=<DivBackward0>)\n",
            "avg_loss step  9500tensor([1.2839], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 9500tensor([1.2854], grad_fn=<DivBackward0>)\n",
            "avg_loss step  10000tensor([1.2825], grad_fn=<DivBackward0>)\n",
            "avg_loss_old step 10000tensor([1.2839], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f7adfbaebc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#loss = loss1 - loss2 - 5*loss3 - loss4 #with PR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mz_semi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mz_semi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mz_semi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_semi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz_semi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-KVqNzvO4Vj2",
        "outputId": "55828bcd-831f-423b-86d5-942ee581a068"
      },
      "source": [
        "# Plotting the semi factual image according to the cf found by the latent dimension loss and target class provided\n",
        "G_img= G(z_semi)\n",
        "npimgs = G_img[0].detach().numpy()\n",
        "#print(npimgs.shape)\n",
        "plt.imshow(npimgs[0],cmap='Greys_r')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a1d12f9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCPEMRJD51uB",
        "outputId": "633ec435-1430-43e5-f33e-857e61864bad"
      },
      "source": [
        "torch.topk(Classifier_model(G(z_semi))[0],2)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([0.5481, 0.4519], grad_fn=<TopkBackward>), indices=tensor([4, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xss6bV4855Xu",
        "outputId": "0f0f807c-8c6c-4877-ebe3-61aa085f5ed9"
      },
      "source": [
        "loss3"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.5944, grad_fn=<LogBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ca1QkMn7qkr",
        "outputId": "bd71487e-4695-4202-afe0-b5ab7d98e7c7"
      },
      "source": [
        "torch.abs((Classifier_model(G(z_semi))[0][4] - Classifier_model(G(z_semi))[0][6]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5481, grad_fn=<AbsBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwi1rqET7vhE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}